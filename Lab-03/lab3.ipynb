{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51eac49",
   "metadata": {},
   "source": [
    "1. Load the dataset into a Pandas DataFrame and extract the text and label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "afa057c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the given CSV file containing text and label columns into a Pandas DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# First column contains review, second column contains the label(Positive, negative, neutral).\n",
    "\n",
    "print(\"Load the given CSV file containing text and label columns into a Pandas DataFrame.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# https://www.kaggle.com/code/akanksha10/sentiment-analysis-dataset/input\n",
    "df = pd.read_csv('test.csv', encoding='latin1')\n",
    " \n",
    "df = df.drop(columns=[col for col in df.columns if col not in ['text', 'sentiment']])\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.drop(df.index[100:])\n",
    "df = df.rename(columns={'sentiment':'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a980397",
   "metadata": {},
   "source": [
    "2. Perform tokenization on all documents and store the tokens corresponding to each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef45fc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed for 100 documents\n",
      "Original text: Last session of the day  http://twitpic.com/67ezh\n",
      "Tokens: ['last', 'session', 'of', 'the', 'day', 'http']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "df['tokens'] = df['text'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "print(f\"Tokenization completed for {len(df)} documents\")\n",
    "print(f\"Original text: {df['text'].iloc[0]}\")\n",
    "print(f\"Tokens: {df['tokens'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270955a0",
   "metadata": {},
   "source": [
    "3. Apply case folding by converting all tokens to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3fc0f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example - Document 1:\n",
      "Original text: Last session of the day  http://twitpic.com/67ezh\n",
      "Tokens: ['last', 'session', 'of', 'the', 'day', 'http']\n",
      "\n",
      "Example - Document 2:\n",
      "Original text:  Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).\n",
      "Tokens: ['shanghai', 'is', 'also', 'really', 'exciting', 'precisely', 'skyscrapers', 'galore', 'good', 'tweeps', 'in', 'china', 'sh', 'bj']\n",
      "\n",
      "Example - Document 3:\n",
      "Original text: Recession hit Veronique Branquinho, she has to quit her company, such a shame!\n",
      "Tokens: ['recession', 'hit', 'veronique', 'branquinho', 'she', 'has', 'to', 'quit', 'her', 'company', 'such', 'a', 'shame']\n",
      "\n",
      "NLTK Tokenization ensures that the tokens are already lowercase\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Example - Document {i+1}:\")\n",
    "    print(f\"Original text: {df['text'].iloc[i]}\")\n",
    "    print(f\"Tokens: {df['tokens'].iloc[i]}\\n\")\n",
    "print(\"NLTK Tokenization ensures that the tokens are already lowercase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a1f43",
   "metadata": {},
   "source": [
    "4. Remove stop-words from the tokenized documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04ef66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label  \\\n",
      "0  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
      "1   Shanghai is also really exciting (precisely -...  positive   \n",
      "2  Recession hit Veronique Branquinho, she has to...  negative   \n",
      "3                                        happy bday!  positive   \n",
      "4             http://twitpic.com/4w75p - I like it!!  positive   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                [last, session, of, the, day, http]   \n",
      "1  [shanghai, is, also, really, exciting, precise...   \n",
      "2  [recession, hit, veronique, branquinho, she, h...   \n",
      "3                                      [happy, bday]   \n",
      "4                                [http, i, like, it]   \n",
      "\n",
      "                            tokens_without_stopwords  \n",
      "0                         [last, session, day, http]  \n",
      "1  [shanghai, also, really, exciting, precisely, ...  \n",
      "2  [recession, hit, veronique, branquinho, quit, ...  \n",
      "3                                      [happy, bday]  \n",
      "4                                       [http, like]  \n",
      "Stop-word removal completed.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load English stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['tokens_without_stopwords'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df.head())\n",
    "print(f\"Stop-word removal completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f7fb2",
   "metadata": {},
   "source": [
    "5. Apply stemming to the filtered tokens and store the stemmed tokens separately.\n",
    "6. Create a new column containing the final preprocessed text after stemming by joining the stemmed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "83eb693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label  \\\n",
      "0  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
      "1   Shanghai is also really exciting (precisely -...  positive   \n",
      "2  Recession hit Veronique Branquinho, she has to...  negative   \n",
      "3                                        happy bday!  positive   \n",
      "4             http://twitpic.com/4w75p - I like it!!  positive   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                [last, session, of, the, day, http]   \n",
      "1  [shanghai, is, also, really, exciting, precise...   \n",
      "2  [recession, hit, veronique, branquinho, she, h...   \n",
      "3                                      [happy, bday]   \n",
      "4                                [http, i, like, it]   \n",
      "\n",
      "                            tokens_without_stopwords  \\\n",
      "0                         [last, session, day, http]   \n",
      "1  [shanghai, also, really, exciting, precisely, ...   \n",
      "2  [recession, hit, veronique, branquinho, quit, ...   \n",
      "3                                      [happy, bday]   \n",
      "4                                       [http, like]   \n",
      "\n",
      "                                      stemmed_tokens  \n",
      "0                         [last, session, day, http]  \n",
      "1  [shanghai, also, realli, excit, precis, skyscr...  \n",
      "2  [recess, hit, veroniqu, branquinho, quit, comp...  \n",
      "3                                      [happi, bday]  \n",
      "4                                       [http, like]  \n",
      "                                                text  \\\n",
      "0  Last session of the day  http://twitpic.com/67ezh   \n",
      "1   Shanghai is also really exciting (precisely -...   \n",
      "2  Recession hit Veronique Branquinho, she has to...   \n",
      "3                                        happy bday!   \n",
      "4             http://twitpic.com/4w75p - I like it!!   \n",
      "\n",
      "                           stemmed_preprocessed_text  \n",
      "0                              last session day http  \n",
      "1  shanghai also realli excit precis skyscrap gal...  \n",
      "2  recess hit veroniqu branquinho quit compani shame  \n",
      "3                                         happi bday  \n",
      "4                                          http like  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "df['stemmed_tokens'] = df['tokens_without_stopwords'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(df.head())\n",
    "\n",
    "df['stemmed_preprocessed_text'] = df['stemmed_tokens'].apply(lambda x: ' '.join(x))\n",
    "print(df[['text', 'stemmed_preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa09df7",
   "metadata": {},
   "source": [
    "7. Construct the Bag-of-Words vocabulary using the stemmed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "44a4fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 435\n",
      "Sample BoW tokens: ['fun', 'papaya', 'today', 'dumbfac', 'breaki', 'everyon', 'bought', 'world', 'know', 'china', 'registr', 'noon', 'man', 'figur', 'beep', 'loo', 'spend', 'sweet', 'return', 'though', 'branquinho', 'explod', 'yesterday', 'eat', 'alway', 'sweden', 'would', 'rain', 'heard', 'tonight']\n"
     ]
    }
   ],
   "source": [
    "token_population_stemmed = [stoken for stokens in df['stemmed_tokens'] for stoken in stokens]\n",
    "\n",
    "# Create unique vocabulary\n",
    "vocab = list(set(token_population_stemmed))\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample BoW tokens: {vocab[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096074c4",
   "metadata": {},
   "source": [
    "8. Determine the size of the stemming-based vocabulary and list the top 10 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b926573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 435\n",
      "\n",
      "Top 10 most common tokens in Bag of Words:\n",
      "Token: day, Frequency: 10\n",
      "Token: go, Frequency: 9\n",
      "Token: happi, Frequency: 7\n",
      "Token: like, Frequency: 6\n",
      "Token: im, Frequency: 6\n",
      "Token: time, Frequency: 6\n",
      "Token: get, Frequency: 6\n",
      "Token: need, Frequency: 6\n",
      "Token: watch, Frequency: 5\n",
      "Token: look, Frequency: 5\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "token_counts = Counter(token_population_stemmed)\n",
    "print(f\"\\nVocabulary size: {len(set(token_population_stemmed))}\")\n",
    "print(\"\\nTop 10 most common tokens in Bag of Words:\")\n",
    "for token, freq in token_counts.most_common(10):\n",
    "    print(f\"Token: {token}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6446f74",
   "metadata": {},
   "source": [
    "9. Represent each document using Bag-of-Words frequency vectors and generate the document–term matrix (DTM) for the stemmed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d3b160f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM shape: (100, 435)\n",
      "(Number of documents: 100, Vocabulary size: 435)\n",
      "\n",
      "First document vector (first 20 dimensions):\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Create unique vocabulary\n",
    "vocab = list(set(token_population_stemmed))\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Vectorize each document based on its own tokens\n",
    "def vectorize_document(tokens):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    token_freq = Counter(tokens)\n",
    "    for token, count in token_freq.items():\n",
    "        if token in vocab_dict:\n",
    "            vector[vocab_dict[token]] = count\n",
    "    return vector\n",
    "\n",
    "# Apply vectorization to all documents using stemmed tokens\n",
    "dtm = np.array([vectorize_document(tokens) for tokens in df['stemmed_tokens']])\n",
    "\n",
    "print(f\"DTM shape: {dtm.shape}\")\n",
    "print(f\"(Number of documents: {dtm.shape[0]}, Vocabulary size: {dtm.shape[1]})\")\n",
    "print(f\"\\nFirst document vector (first 20 dimensions):\\n{dtm[0][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731cd5f",
   "metadata": {},
   "source": [
    "10. Apply lemmatization to the filtered tokens and store the lemmatized tokens separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac9f902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label  \\\n",
      "0  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
      "1   Shanghai is also really exciting (precisely -...  positive   \n",
      "2  Recession hit Veronique Branquinho, she has to...  negative   \n",
      "3                                        happy bday!  positive   \n",
      "4             http://twitpic.com/4w75p - I like it!!  positive   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                [last, session, of, the, day, http]   \n",
      "1  [shanghai, is, also, really, exciting, precise...   \n",
      "2  [recession, hit, veronique, branquinho, she, h...   \n",
      "3                                      [happy, bday]   \n",
      "4                                [http, i, like, it]   \n",
      "\n",
      "                            tokens_without_stopwords  \\\n",
      "0                         [last, session, day, http]   \n",
      "1  [shanghai, also, really, exciting, precisely, ...   \n",
      "2  [recession, hit, veronique, branquinho, quit, ...   \n",
      "3                                      [happy, bday]   \n",
      "4                                       [http, like]   \n",
      "\n",
      "                                      stemmed_tokens  \\\n",
      "0                         [last, session, day, http]   \n",
      "1  [shanghai, also, realli, excit, precis, skyscr...   \n",
      "2  [recess, hit, veroniqu, branquinho, quit, comp...   \n",
      "3                                      [happi, bday]   \n",
      "4                                       [http, like]   \n",
      "\n",
      "                           stemmed_preprocessed_text  \\\n",
      "0                              last session day http   \n",
      "1  shanghai also realli excit precis skyscrap gal...   \n",
      "2  recess hit veroniqu branquinho quit compani shame   \n",
      "3                                         happi bday   \n",
      "4                                          http like   \n",
      "\n",
      "                                   lemmatized_tokens  \n",
      "0                         [last, session, day, http]  \n",
      "1  [shanghai, also, really, exciting, precisely, ...  \n",
      "2  [recession, hit, veronique, branquinho, quit, ...  \n",
      "3                                      [happy, bday]  \n",
      "4                                       [http, like]  \n",
      "Lemmatization completed.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['lemmatized_tokens'] = df['tokens_without_stopwords'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(df.head())\n",
    "print(f\"Lemmatization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda13505",
   "metadata": {},
   "source": [
    "11. Create a new column containing the final preprocessed text after lemmatization by joining the lemmatized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26d84c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Last session of the day  http://twitpic.com/67ezh   \n",
      "1   Shanghai is also really exciting (precisely -...   \n",
      "2  Recession hit Veronique Branquinho, she has to...   \n",
      "3                                        happy bday!   \n",
      "4             http://twitpic.com/4w75p - I like it!!   \n",
      "\n",
      "                        lemmatized_preprocessed_text  \n",
      "0                              last session day http  \n",
      "1  shanghai also really exciting precisely skyscr...  \n",
      "2  recession hit veronique branquinho quit compan...  \n",
      "3                                         happy bday  \n",
      "4                                          http like  \n"
     ]
    }
   ],
   "source": [
    "df['lemmatized_preprocessed_text'] = df['lemmatized_tokens'].apply(lambda x: ' '.join(x))\n",
    "print(df[['text', 'lemmatized_preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3fcbf",
   "metadata": {},
   "source": [
    "12. Construct the Bag-of-Words vocabulary using the lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "971fd22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization-based Vocabulary size: 453\n",
      "Sample BoW tokens: ['fun', 'papaya', 'today', 'watched', 'bought', 'sorry', 'world', 'know', 'every', 'china', 'noon', 'man', 'beep', 'loo', 'spend', 'sweet', 'return', 'though', 'branquinho', 'yesterday', 'eat', 'sweden', 'would', 'rain', 'heard', 'corky', 'tonight', 'left', 'gum', 'x']\n"
     ]
    }
   ],
   "source": [
    "token_population_lemmatized = [ltoken for ltokens in df['lemmatized_tokens'] for ltoken in ltokens]\n",
    "\n",
    "# Create unique vocabulary\n",
    "vocab_lemma = list(set(token_population_lemmatized))\n",
    "\n",
    "print(f\"\\nLemmatization-based Vocabulary size: {len(vocab_lemma)}\")\n",
    "print(f\"Sample BoW tokens: {vocab_lemma[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d164231",
   "metadata": {},
   "source": [
    "13. Determine the size of the lemmatization-based vocabulary and list the top 10 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "93c46c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization-based Vocabulary size: 453\n",
      "\n",
      "Top 10 most common lemmatized tokens:\n",
      "Token: day, Frequency: 10\n",
      "Token: happy, Frequency: 7\n",
      "Token: like, Frequency: 6\n",
      "Token: im, Frequency: 6\n",
      "Token: time, Frequency: 6\n",
      "Token: go, Frequency: 6\n",
      "Token: u, Frequency: 5\n",
      "Token: need, Frequency: 5\n",
      "Token: know, Frequency: 5\n",
      "Token: http, Frequency: 4\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_counts_lemma = Counter(token_population_lemmatized)\n",
    "print(f\"\\nLemmatization-based Vocabulary size: {len(set(token_population_lemmatized))}\")\n",
    "print(\"\\nTop 10 most common lemmatized tokens:\")\n",
    "for token, freq in token_counts_lemma.most_common(10):\n",
    "    print(f\"Token: {token}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40df6c2",
   "metadata": {},
   "source": [
    "14. Represent each document using Bag-of-Words frequency vectors and generate the document–term matrix (DTM) for the lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fab8de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM shape: (100, 453)\n",
      "(Number of documents: 100, Vocabulary size: 453)\n",
      "\n",
      "First document vector (first 20 dimensions):\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Create unique vocabulary\n",
    "vocab_lemma = list(set(token_population_lemmatized))\n",
    "vocab_dict_lemma = {word: idx for idx, word in enumerate(vocab_lemma)}\n",
    "\n",
    "# Vectorize each document\n",
    "def vectorize_document_lemma(tokens):\n",
    "    vector = np.zeros(len(vocab_lemma))\n",
    "    token_freq = Counter(tokens)\n",
    "    for token, count in token_freq.items():\n",
    "        if token in vocab_dict_lemma:\n",
    "            vector[vocab_dict_lemma[token]] = count\n",
    "    return vector\n",
    "\n",
    "# Create DTM for lemmatized tokens\n",
    "dtm_lemma = np.array([vectorize_document_lemma(tokens) for tokens in df['lemmatized_tokens']])\n",
    "\n",
    "print(f\"DTM shape: {dtm_lemma.shape}\")\n",
    "print(f\"(Number of documents: {dtm_lemma.shape[0]}, Vocabulary size: {dtm_lemma.shape[1]})\")\n",
    "print(f\"\\nFirst document vector (first 20 dimensions):\\n{dtm_lemma[0][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872aa2db",
   "metadata": {},
   "source": [
    "15. Select any one document and display its BoW vector from:\n",
    "- the stemming-based DTM\n",
    "- the lemmatization-based DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a059be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 BoW vector (Stemming-based DTM):\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "\n",
      "Document 1 BoW vector (Lemmatization-based DTM):\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDocument 1 BoW vector (Stemming-based DTM):\")\n",
    "print(dtm[5])\n",
    "print(\"\\nDocument 1 BoW vector (Lemmatization-based DTM):\")\n",
    "print(dtm_lemma[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4938d51",
   "metadata": {},
   "source": [
    "16. Identify words that appear in most documents and words that appear in very few documents, and discuss their significance in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6a644219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common tokens in the stemming-based BoW:\n",
      "Token: day, Frequency: 10\n",
      "Token: go, Frequency: 9\n",
      "Token: happi, Frequency: 7\n",
      "Token: like, Frequency: 6\n",
      "Token: im, Frequency: 6\n",
      "Token: time, Frequency: 6\n",
      "Token: get, Frequency: 6\n",
      "Token: need, Frequency: 6\n",
      "Token: watch, Frequency: 5\n",
      "Token: look, Frequency: 5\n",
      "\n",
      "10 most common tokens in the lemmatization-based BoW:\n",
      "Token: day, Frequency: 10\n",
      "Token: happy, Frequency: 7\n",
      "Token: like, Frequency: 6\n",
      "Token: im, Frequency: 6\n",
      "Token: time, Frequency: 6\n",
      "Token: go, Frequency: 6\n",
      "Token: u, Frequency: 5\n",
      "Token: need, Frequency: 5\n",
      "Token: know, Frequency: 5\n",
      "Token: http, Frequency: 4\n",
      "\n",
      "10 least common tokens in the stemming-based BoW:\n",
      "Token: class, Frequency: 1\n",
      "Token: haaaw, Frequency: 1\n",
      "Token: deadlin, Frequency: 1\n",
      "Token: uk, Frequency: 1\n",
      "Token: noon, Frequency: 1\n",
      "Token: meet, Frequency: 1\n",
      "Token: afternoon, Frequency: 1\n",
      "Token: return, Frequency: 1\n",
      "Token: decid, Frequency: 1\n",
      "Token: tri, Frequency: 1\n",
      "\n",
      "10 least common tokens in the lemmatization-based BoW:\n",
      "Token: class, Frequency: 1\n",
      "Token: haaaw, Frequency: 1\n",
      "Token: deadline, Frequency: 1\n",
      "Token: uk, Frequency: 1\n",
      "Token: noon, Frequency: 1\n",
      "Token: meet, Frequency: 1\n",
      "Token: afternoon, Frequency: 1\n",
      "Token: return, Frequency: 1\n",
      "Token: decide, Frequency: 1\n",
      "Token: trying, Frequency: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"10 most common tokens in the stemming-based BoW:\")\n",
    "from collections import Counter\n",
    "token_counts_stem = Counter(token_population_stemmed)\n",
    "for token, freq in token_counts_stem.most_common(10):\n",
    "    print(f\"Token: {token}, Frequency: {freq}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"10 most common tokens in the lemmatization-based BoW:\")\n",
    "token_counts_lemma = Counter(token_population_lemmatized)\n",
    "for token, freq in token_counts_lemma.most_common(10):\n",
    "    print(f\"Token: {token}, Frequency: {freq}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"10 least common tokens in the stemming-based BoW:\")\n",
    "for token, freq in token_counts_stem.most_common()[:-11:-1]:\n",
    "    print(f\"Token: {token}, Frequency: {freq}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"10 least common tokens in the lemmatization-based BoW:\")\n",
    "for token, freq in token_counts_lemma.most_common()[:-11:-1]:\n",
    "    print(f\"Token: {token}, Frequency: {freq}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afaabe6",
   "metadata": {},
   "source": [
    "These words show that both (stemming-based and lemmatization-based methods) capture meaning well. The very common and very rare words are the same across both the text representations. Most of the words are neutral and do not contribute to sentiment classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
